
# CIP [0017]: Integrate Plumo Proofs into the lightest sync protocol

- Date: 2020-07-27
- Author: @lucasege
- Status: WIP

## Overview

In order to continue supporting the development of a seamless mobile sync experience, the lightest sync protocol should be extended to support Plumo Zk-SNARKs which can quickly verify transitions from epoch X to epoch X + Y, where Y could be on the order of 10s-100s of epochs.

The prover/verifier for Plumo proofs have already been implemented, and the exposed go interface is available [here](https://github.com/celo-org/celo-bls-go)

More information on Plumo can be found in the [reference paper](https://docs.zkproof.org/pages/standards/accepted-workshop3/proposal-plumo_celolightclient.pdf)

## Goals

- Support adding/storing proofs on full nodes
- Gossip these proofs to propagate them through the network
- Extend the `les` protocol such that the server will provide these proofs in place of headers (when appropriate), and the client will consume/verify proofs accordingly

## Proposed Solution

### Full node changes (RPC and `eth` protocol)
We propose to add two new RPCs: 
* `AddProof(proof []byte, firstEpoch uint, lastEpoch uint, versionNumber uint)`
* `Proofs() ([][]byte)`.

`versionNumber` here refers to the snark circuit version, for more info refer to #upgradeability below.

When an eventual "proving service" generates a new proof it will publish this proof to a full node that it is running, which will then propagate the proof throughout the network.

This `AddProof` function will then utilize the `verificationKey` corresponding to the `versionNumber` and the corresponding validator sets for these epochs to verify the proof. If successful, it will then store the proof in a new database `plumoproofdata`, keyed by the combination of its first and last epoch and the proof's version number. 

To propagate these proofs throughout the network, we also propose to extend the `eth` protocol with three new message types: 
```
NewPlumoProof (0x019)
[[firstEpoch, lastEpoch, versionNumber], ...]
Specify one or more new plumo proofs which have appeared on the network.
This indicates to other peers that a new proof is available by sending out its metadata. Peers are then expected to send a `GetPlumoProofsMsg` to request this proof by its metadata.

GetPlumoProofsMsg (0x1a)
[complement: Boolean, [[firstEpoch, lastEpoch, versionNumber], ...]]
Sent from node A to node B: if `complement` is true, then this message is sent with the set of proofs
that node A currently *has* and requests those it is missing.
Node B's reply must contain any proofs that node B has that were not included in the message node A sent.
If `complement` is false, then this message is sent with the set of proofs that node A currently
*does not have* and requests these from this peer.
Node B's reply must contain the proofs requested, if node B has them. Node A should only request proofs
that node B has indicated via `NewPlumoProof` that it has.


PlumoProofMsg (0x1b)
[plumoProof_0, plumoProof_1, ...]
Reply to GetPlumoProofMsg. The items in the list are plumo proof structs, {proof: []byte, firstEpoch: uint, lastEpoch: uint, versionNumber: uint}.
This may validly contain no plumo proofs if the GetPlumoProofMsg included all proofs this node is aware of.
```

This will be accompanied by a new `proofFetcher` object to be in charge of fetching these proofs from peers while providing protections against DoS attacks.

### `les` protocol changes

Relevant proposed structs:
```
// Same as in the `eth` protocol, minimal proof metadata
ProofMetadata struct {
  FirstEpoch uint
  LastEpoch uint
  VersionNumber uint
}

// Metadata for each epoch to construct `EpochBlock` for the snark's verify fn
// May be unnecessary if we can make assumptions on `MaxNonSigners`
ProofInputs struct {
  Index         uint // Epoch Index 
  MaxNonSigners uint // MinQuorumSize()
}

// All of the data needed by the light client to verify and utilize a proof.
// This is also crafted to allow for batch verifications using `FirstHashToField`
// My rough calculations show this struct is ~439 bytes for a non-batched proof with no val set differences.
LightPlumoProof struct {
  Proof             []byte // Serialized proof. Latest estimate from @psi = 383 bytes
  FirstEpoch        LightEpochBlock // First epoch this proof is over
  LastEpoch         LightEpochBlock // Last epoch this proof is over
  VersionNumber     uint // Proof version number, indicates the corresponding verification key to use
  LastHashToField  []byte  // Hash to field of Last epoch's `snark.EpochBlock` struct. To be used for batch verification.
                           // If there is no batch verification, this will be empty. May need an additional boolean to signify this.
                           // If batch verification should be used, newValidators and deletedValidators will be empty.
  NewValidators     []blsCrypto.SerializedPublicKey // ValSetDiff(FirstEpoch, LastEpoch), added validators
  DeletedValidators *big.Int // ValSetDiff(FirstEpoch, LastEpoch): bitmap of deleted validators from the `firstEpoch` valset.
}
```

The light clients will need to store verification keys corresponding to the existing versions of the snark circuit, as elaborated in #upgradeability. 

There are two proposed extensions for the `les` protocol:

Option 1:
This option is more resilient to incomplete proof propagation for full nodes, but requires an extra round trip of messages to achieve this. The idea is for the client to query the server to see what proofs are available, and then request the optimal proofs. If the client wants to minimize network traffic and assume proofs are well distributed, they can also bypass the `GetAvailableProofs` step and just request that the server sends the best proofs it has for the client's current epoch (using the `handleLogic` flag on `GetEpochHeadersOrProofs`).
```
GetAvailableProofs - Asks the server what proofs it has. Server responds with `[]proofMetadata` of all available proofs.
GetEpochHeadersOrProofs:
  Client sends (highestEpoch uint, handleLogic boolean, requestedProofs []proofMetadata)
    `highestEpoch` is the current highestEpoch the client knows of.
    `handleLogic`: If true, indicates the server should send back the best set of proofs/headers it has
                   If false, the server should look at `requestedProofs` and send back proofs requested, with headers to fill gaps as appropriate.
    `requestedProofs`: Set of proofs' metadata requested by the client
  Server responds with `EpocHeadersOrProofs` dependent on `handleLogic`.
  Both responses are subject to message size limits limiting the number of proofs/headers.
EpochHeadersOrProofs: Contains the set of proofs & headers to help the client, as requested in `GetEpochHeadersOrProofs`.
```

Option 2: This option benefits from assumptions on the pattern of proof generation (likely at least one for every ~120 epochs) and by assuming proofs are well distributed (all full nodes have all proofs). The idea here is to pass a bitmap to the `GetEpochHeadersOrProofs` msg that indicates if the client knows the validator set for the past `SIZE_OF_BITMAP` (~240?) epochs. This allows the full node to optimize the set of proofs sent back, while minimizing computation for the client. The main win here would be in a case like this: the client uses a proof to sync to epoch 120, then syncs further to epoch 121, so it knows the validator set of 120 and 121. Then the client goes down for > 120 epochs, and when it comes back the server can figure out to send a proof from 120-240 that we assume is generated, rather than sending 119 epoch headers to verify.

The msgs are:
```
GetEpochHeadersOrProofs:
  Client sends (highestEpoch uint, knownValSets *big.Int)
    `knownValSets` here would be a bitmap of the last ~240 or so epochs that the client knows the val set for
  Server computes the optimal path of proofs & headers that the client can use to sync fastest and send them back using `EpochHeadersOrProofs`.
EpochHeadersOrProofs:
  Server sends this back from `GetEpochHeadersOrProofs` with the optimal set of proofs and headers this client can utilize.
```  

It may be preferred to combine both options, and send a bitmap with `handleLogic == false` in option 1.

Regardless of the option chosen, the `les` client must then be modified to request proofs & headers during `lightest` sync. This will likely replace the function `getEpochOrNormalHeaders` within `eth/downloader`. This will also involve a new function to update the underlying validator set and blockchain data, which will be built to work with checkpoints as well.

Upon receiving proofs and headers in a response message, the light client will verify headers the same way as before, and will verify proofs via a call into `celo-bls-go`'s `verify` fn.

The verify function requires information on an epoch's validator set, so this will likely require some access changes (private -> public) and exposing more data structures to allow the validator set to be queried and modified upon proof verification.

### Upgradeability
In order to support an upgradeable snark circuit, we utilize a `versionNumber` parameter with every proof. This corresponds to a version of the snark circuit and thus a corresponding verification key. The clients are expected to be shipped with all available verification keys, and can be upgraded to receive added verification keys.

For any uses of the snark's `verify` fn in the above protocols, a client is expected to:
* Fetch the verification key corresponding to the proof's `versionNumber` from its storage
* The client then uses that verification key to call `verify` on the proof.

### Bad proofs
Bad proofs should be prevented from propagated throughout the network via the `verify` check that each node performs. In the case that any participant receives a proof that does not pass verification for its inputs, that participant is expected to drop the sender. 

## Alternative Solutions

Alternatively, `GetPlumoProofMsg` could utilize a bloom filter instead, allowing for a constant sized message. However, bloom filters allow for false positives: a false positive here would indicate to a peer that the sender has a proof when the sender does not. This could prevent certain proofs from ever getting to some nodes. For example, say node A is on the network and has received a certain amount of proofs via traditional propagation. Node A then goes down for a short period, and within that period proof P is added and propagated. Node A then comes back online, and sends a `GetPlumoProofsMsg` to request any proofs it may have missed, and sends along the corresponding bloom filter for the proofs it holds. If proof P happens to be a false positive for the set of proofs node A already has, then all of node A's peers will simply return nothing. Thus node A will never receive proof P. This can then propagate similarly through the network either through a similar path as node A, or for any nodes where all of their peers have already "lost" proof P.

`GetPlumoProofMsg` could also request *all* plumo proofs from the receiver, leaving the responsibility of filtering seen proofs with the caller. This is a simpler implementation, but is less scalable as we increase the amount of stored proofs. 

Specific aspects of the `les` changes may have tradeoffs that I've yet to encounter, so any input and feedback on things to consider there would be helpful.

## Risks

Altering the `eth` protocol introduces some risk of introducing bugs, which could inhibit a nodes ability to connect to the network.

Introducing bugs to mobile client syncing. Possible effects of a faulty implementation include not being able to sync to the network, and introducing a security vulnerability which would allow the node to sync to an attack controlled chain. Bugs will be mitigated via chaos-testing (malicious testers), fuzz testing, e2e tests and strong unit tests.

## Useful Links
- [Celo snark go interface](https://github.com/celo-org/celo-bls-go)
- [Celo snark rust implementation](https://github.com/celo-org/celo-bls-snark-rs)
- [Underlying math/crypto library, zexe](https://github.com/scipr-lab/zexe)

## Implementation

WIP https://github.com/celo-org/celo-blockchain/pull/1116
