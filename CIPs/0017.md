
# CIP [0017]: Integrate Plumo Proofs into the lightest sync protocol

- Date: 2020-07-27
- Author: @lucasege
- Status: WIP

## Overview

In order to continue supporting the development of a seamless mobile sync experience, the lightest sync protocol should be extended to support Plumo Zk-SNARKs which can quickly verify transitions from epoch X to epoch X + Y, where Y could be on the order of 10s-100s of epochs.

The prover/verifier for Plumo proofs have already been implemented, and the exposed go interface is available [here](https://github.com/celo-org/celo-bls-go)

More information on Plumo can be found in the [reference paper](https://docs.zkproof.org/pages/standards/accepted-workshop3/proposal-plumo_celolightclient.pdf)

## Goals

- Support adding/storing proofs on full nodes
- Gossip these proofs to propagate them through the network
- Extend the `les` protocol such that the server will provide these proofs in place of headers (when appropriate), and the client will consume/verify proofs accordingly

## Proposed Solution

We propose to add two new RPCs: 
* `AddProof(proof []byte, firstEpoch uint, lastEpoch uint, versionNumber uint)`
* `Proofs() ([][]byte)`.

`versionNumber` here refers to the snark circuit version, for more info refer to #upgradeability below.

When an eventual "proving service" generates a new proof it will publish this proof to a full node that it is running, which will then propagate the proof throughout the network.

This `AddProof` function will then utilize the `verificationKey` corresponding to the `versionNumber` and the corresponding validator sets for these epochs to verify the proof. If successful, it will then store the proof in a new database `plumoproofdata`, keyed by the combination of its first and last epoch. 

To propagate these proofs throughout the network, we also propose to extend the `eth` protocol with three new message types: 
```
NewPlumoProof (0x019)
[[firstEpoch, lastEpoch, versionNumber], ...]
Specify one or more new plumo proofs which have appeared on the network.
This is intended to be honored with a proceeding GetPlumoProofsMsg to request the proof.

GetPlumoProofsMsg (0x1a)
[complement: Boolean, [[firstEpoch, lastEpoch, versionNumber], ...]]
Sent from node A to node B: if `complement` is true, then this message is sent with the set of proofs
that node A currently *has* and requests those it is missing.
Node B's reply must contain any proofs that node B has that were not included in the message node A sent.
If `complement` is false, then this message is sent with the set of proofs that node A currently
*does not have* and requests these from this peer.
Node B's reply must contain the proofs requested, if node B has them. Node A should only request proofs
that node B has indicated via `NewPlumoProof` that it has.


PlumoProofMsg (0x1b)
[plumoProof_0, plumoProof_1, ...]
Reply to GetPlumoProofMsg. The items in the list are plumo proof structs, {proof: []byte, firstEpoch: uint, lastEpoch: uint, versionNumber: uint}.
This may validly contain no plumo proofs if the GetPlumoProofMsg included all proofs this node is aware of.
```

This will likely be accompanied by a new `proofFetcher` object to be in charge of fetching these proofs from peers while providing protections against DoS attacks. This feels like overkill, but seems necessary to prevent attacks and maintain state.

The `les` protocol must then also be extended via at least two messages:
```
GetCertifiedEpochsValidatorSet - Sends the current highest epoch known to the server. Server responds with the minimal set of proofs & headers to give the light client the fastest sync time.
CertifiedEpochsValidatorSet - Receives a mix of header(s) and proof(s) from the server.
```

The `les` client must then be modified to send a `GetCertifiedEpochsValidatorSet` message during `lightest` sync. This will likely replace the function `getEpochOrNormalHeaders` within `eth/downloader`.

Upon receiving the `CertifiedEpochsValidatorSet` response message, the light client must then verify headers and proofs provided. Proofs will be verified via a call into `celo-bls-go`'s `verify` fn.

The verify function requires information on an epoch's validator set, so this will likely require some access changes (private -> public) and exposing more data structures to allow the validator set to be queried and modified upon proof verification.

### Upgradeability
In order to support an upgradeable snark circuit, we utilize a `versionNumber` parameter with every proof. This corresponds to a version of the snark circuit and thus a corresponding verification key. The clients are expected to be shipped with all available verification keys, and can be upgraded to receive added verification keys.

For any uses of the snark's `verify` fn in the above protocols, a client is expected to:
* Fetch the verification key corresponding to the proof's `versionNumber` from its storage
* The client then uses that verification key to call `verify` on the proof.

### Bad proofs
Bad proofs should be prevented from propagated throughout the network via the `verify` check that each node performs. In the case that any participant receives a proof that does not pass verification for its inputs, that participant is expected to drop the sender. 

## Alternative Solutions

Alternatively, `GetPlumoProofMsg` could utilize a bloom filter instead, allowing for a constant sized message. However, bloom filters allow for false positives: a false positive here would indicate to a peer that the sender has a proof when the sender does not. This could prevent certain proofs from ever getting to some nodes. For example, say node A is on the network and has received a certain amount of proofs via traditional propagation. Node A then goes down for a short period, and within that period proof P is added and propagated. Node A then comes back online, and sends a `GetPlumoProofsMsg` to request any proofs it may have missed, and sends along the corresponding bloom filter for the proofs it holds. If proof P happens to be a false positive for the set of proofs node A already has, then all of node A's peers will simply return nothing. Thus node A will never receive proof P. This can then propagate similarly through the network either through a similar path as node A, or for any nodes where all of their peers have already "lost" proof P.

`GetPlumoProofMsg` could also request *all* plumo proofs from the receiver, leaving the responsibility of filtering seen proofs with the caller. This is a simpler implementation, but is less scalable as we increase the amount of stored proofs. 

Specific aspects of the `les` changes may have tradeoffs that I've yet to encounter, so any input and feedback on things to consider there would be helpful.

## Risks

Altering the `eth` protocol introduces some risk of introducing bugs, which could inhibit a nodes ability to connect to the network.

Introducing bugs to mobile client syncing. Possible effects of a faulty implementation include not being able to sync to the network, and introducing a security vulnerability which would allow the node to sync to an attack controlled chain. Bugs will be mitigated via chaos-testing (malicious testers), fuzz testing, e2e tests and strong unit tests.

## Useful Links
- [Celo snark go interface](https://github.com/celo-org/celo-bls-go)
- [Celo snark rust implementation](https://github.com/celo-org/celo-bls-snark-rs)
- [Underlying math/crypto library, zexe](https://github.com/scipr-lab/zexe)

## Implementation

WIP https://github.com/celo-org/celo-blockchain/pull/1116
